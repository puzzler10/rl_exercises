{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ch5 Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 5 talks about monte carlo methods. Monte carlo methods work roughly like this: \n",
    "* simulate an episode  \n",
    "* go through states in the episode and find the return of each one \n",
    "* average together returns for each state across episodes to get an estimate of the state's value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do monte carlo methods differ from dynamic programming and temporal-difference? \n",
    "\n",
    "**DP algorithms** assume a model of the environment. To use DP algorithms, you use the Bellman equations, which in turn require $\\pi(a|s)$ , $p(s'|s,a)$ and $r(s,a,s')$. These last two are only known if you have a model of the environment. Essentially, the value of a state is estimated by considering all the states around it and the chance that the agent ends up there. \n",
    "With DP algorithms, you typically can use either $v_\\pi (s)$ or $q_\\pi (s,a)$. DP algorithms bootstrap: the estimate for one state ($v(s_1)$) is used to update the estimate for other states (e.g. $v(s_2)$)\n",
    "\n",
    "**MC algorithms** are used when you don't have a model of the environment, and hence you don't know $p(s'|s,a)$ and $r(s,a,s')$. MC models can create an estimate of the dynamics of the environment through running lots of episodes and averaging the results. Typically you use the action-value function $q(s,a)$ instead of $v(s)$ for MC methods.  \n",
    "\n",
    "**TD algorithms**, like MC algorithms, are used when there isn't a model of the environment. The difference between the two is that TD algorithms don't wait until the end of the episode before updating value functions: rather, they can do it at the end of every time step. \n",
    "\n",
    "If you only care about the value of one state $s$, with MC/TD methods you can just start lots of simulations from that one point and average the results. DP methods need a good estimate of the states around $s$ to create a good estimate for $v(s)$, and to create a good estimate for those states, you need the states around them, and so on. This leads to the property that you cannot just estimate the value of one state with a DP algorithm: you must estimate the complete system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On vs off policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two broad classes of algorithms: on-policy and off-policy\n",
    "* **On-policy** algorithms optimise a policy and also use it for selecting actions\n",
    "* **Off-policy** algorithms use one policy for selecting actions and optimise a different policy\n",
    "\n",
    "Here's an example: let's say we have two policies, $b$ and $\\pi$. An off-policy method would choose to move around with $b$ and seek to optimise $\\pi$ as the optimal policy. Why would we want to do this? Say that the policy $b$ is the equiprobable random policy - it just moves around at random, and that the policy $\\pi$ is close to optimal. Moving with policy $\\pi$ might lead us to not explore the space enough (the exploration problem), and with policy $b$ we can have sufficient exploration. We don't lose anything either, because with the results of exploration we can update the optimal policy $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you move around with policy $b$, how do you calculate the value of states with respect to a different policy, $\\pi$? You need to 'weight' the results you get using importance sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
